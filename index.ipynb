{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Exploration\n",
    "![](https://media.giphy.com/media/3o6ozjrPeWQifzyA6Y/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This morning we will be doing some EDA with the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.collocations import (BigramCollocationFinder, BigramAssocMeasures, \n",
    "                               TrigramCollocationFinder, TrigramAssocMeasures)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we import the policy proposal by 2020 Democratic Presidential Candidates Bernie Sanders and Elizabeth Warren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>policy</th>\n",
       "      <th>candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100% Clean Energy for America</td>\n",
       "      <td>As published on Medium on September 3rd, 2019:...</td>\n",
       "      <td>warren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A Comprehensive Agenda to Boost America’s Smal...</td>\n",
       "      <td>Small businesses are the heart of our economy....</td>\n",
       "      <td>warren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A Fair and Welcoming Immigration System</td>\n",
       "      <td>As published on Medium on July 11th, 2019:\\nIm...</td>\n",
       "      <td>warren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A Fair Workweek for America’s Part-Time Workers</td>\n",
       "      <td>Working families all across the country are ge...</td>\n",
       "      <td>warren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A Great Public School Education for Every Student</td>\n",
       "      <td>I attended public school growing up in Oklahom...</td>\n",
       "      <td>warren</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                      100% Clean Energy for America   \n",
       "1  A Comprehensive Agenda to Boost America’s Smal...   \n",
       "2            A Fair and Welcoming Immigration System   \n",
       "3    A Fair Workweek for America’s Part-Time Workers   \n",
       "4  A Great Public School Education for Every Student   \n",
       "\n",
       "                                              policy candidate  \n",
       "0  As published on Medium on September 3rd, 2019:...    warren  \n",
       "1  Small businesses are the heart of our economy....    warren  \n",
       "2  As published on Medium on July 11th, 2019:\\nIm...    warren  \n",
       "3  Working families all across the country are ge...    warren  \n",
       "4  I attended public school growing up in Oklahom...    warren  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/2020_policies_feb_24.csv', index_col = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T21:21:46.525652Z",
     "start_time": "2020-07-14T21:21:46.522008Z"
    }
   },
   "source": [
    "**We need to do some processing to make this text usable.** \n",
    "\n",
    "In the cell below, define a function called `prepocessing` that receives a single parameter called `text`.\n",
    "\n",
    "<u><b>This function should:</b></u>\n",
    "1. Lower the text so all letters are the same case\n",
    "2. Use nltk's `word_tokenize` function to convert the string into a list of tokens.\n",
    "3. Remove stop words from the data using nltk's english stopwords corpus.\n",
    "4. Use nltk's `PortStemmer` to stem the text data\n",
    "5. Remove punctuation from the data \n",
    "    - *(You can use the [string](https://www.journaldev.com/23788/python-string-module) library for this)*\n",
    "6. Convert the list of tokens into a string\n",
    "7. Remove opening and trailing spaces, and replace all double spaces with a single space.\n",
    "8. Return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "def preprocessing(text):\n",
    "    data = text.lower()\n",
    "    data = word_tokenize(data)\n",
    "    data = [word for word in data if word not in stops]\n",
    "    data = [stemmer.stem(token) for token in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this warmup, tests are not provided.** \n",
    "\n",
    "Instead, examine the output for the following cell. \n",
    "- Was your code successful? \n",
    "- Are there words in the output that should be added to our list of stopwords?\n",
    "- Should we remove numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(df.policy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's apply our preprocessing to every policy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.policy = df.policy.apply(preprocessing)\n",
    "\n",
    "print(df.policy[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can explore our text data.\n",
    "\n",
    "In the cell below define a function called `average_word_length` that receives a single parameter called `text`, and outputs the average word length.\n",
    "\n",
    "<u><b>This function should:</b></u>\n",
    "1. Split the text into a list of tokens\n",
    "2. Find the length of every word in the list\n",
    "3. Sum the word lengths and divide by the number of words in the list of tokens.\n",
    "4. Return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def average_word_length(text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply our function to every policy and add the output as column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_word_length'] = df.policy.apply(average_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet let's take a look at the documents with the highest average word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='average_word_length', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An average measurement can be a bit misleading. \n",
    "\n",
    "Let's also write a function that finds the word count for a given document.\n",
    "\n",
    "In the cell below, define a function called `word_count` that receives a single `text` parameter.\n",
    "\n",
    "<u><b>This function should:</b></u>\n",
    "1. Split the text data\n",
    "2. Return the length of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def word_count(text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Now we apply the function to our entire dataset, and save the output as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df.policy.apply(word_count)\n",
    "\n",
    "df.sort_values(by='average_word_length', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. Let's take a look at the distribution for the `word_count` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warren_df = df[df.candidate=='warren']\n",
    "sanders_df = df[df.candidate=='sanders']\n",
    "\n",
    "plt.hist(warren_df.word_count, alpha=.6, label='Warren')\n",
    "plt.hist(sanders_df.word_count, alpha=.6, label='Sanders')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the average length of a policy is about 1,000 words.\n",
    "\n",
    "Let's print the mean and median for the `word_count` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Word Count: ',df.word_count.mean())\n",
    "print('Median Word Count: ',df.word_count.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are some outliers in this data in a full data science project would need to be explored.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, final thing!**\n",
    "\n",
    "Let's find out what the most frequent words are for each candidate.\n",
    "\n",
    "First, we use list comprehension to create a list of token-lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_warren= [word_tokenize(policy) for policy in warren_df.policy] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to create a bag of words. AKA a single list containing every token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T02:02:40.284467Z",
     "start_time": "2020-07-15T02:02:40.262652Z"
    }
   },
   "outputs": [],
   "source": [
    "warren_bow = []\n",
    "for doc in token_warren:\n",
    "    warren_bow.extend([word.lower() for word in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the `FreqDist` object to find the 10 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_warren = FreqDist(warren_bow)\n",
    "print(fd_warren.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any words here that should be added to our list of stopwords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the cell below* define a function called `word_frequency` that receives a series of documents, and outputs a fitted FreqDist object.\n",
    "\n",
    "<u><b>This function should be</b></u> a generalized version of the code we just wrote, only instead of printing out the most frequent words, the function should return the `fd` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def word_frequency(documents):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can feed all of sanders policies into our `word_frequency` functions and receive a fitted `FreqDist` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_sanders = word_frequency(sanders_df.policy)\n",
    "fd_sanders.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FreqDist` objects come with a handy `.plot` method :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T01:53:50.258743Z",
     "start_time": "2020-07-15T01:53:50.102070Z"
    }
   },
   "outputs": [],
   "source": [
    "fd_sanders.plot(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
